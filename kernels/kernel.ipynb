{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6a8376b4f432d323d023d8a938ed56fecf352f2"
   },
   "source": [
    "# Predicting Housing Prices in Ames, Iowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22080ccd36043ff0dd260cfcde044a7ee474aadb"
   },
   "source": [
    "The goal of this exercise to build a housing price predictor for the Ames, Iowa dataset. The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's a wonderful alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48ac98ee10626fcfc4f1edc83c627a3e370c981d"
   },
   "source": [
    "## Part 1: Preliminary checks on file and loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4622d34d5253f625721574d3856446b9610fd2a5"
   },
   "source": [
    "It's always good to do a few preliminary checks such as file size, line numbers, and printing the first few lines to not load corrupt data or exceed one's memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a5e604e7-3c20-409b-ad29-5c2a4fe40738",
    "_uuid": "44b399828f0b07fe63abbdcdf74bbf3b22bb8067"
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "iowa_file = '../input/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "f50092271c3111ba40a041c3805fe2df37f9e7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size in MB:\n",
      "0.4393348693847656\n"
     ]
    }
   ],
   "source": [
    "print('File size in MB:')\n",
    "print(path.getsize(iowa_file) / (1<<20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b5c1319fc0083b44297b293ca59bad418d694324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in file:\n",
      "1461\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows in file:')\n",
    "with open(iowa_file) as file:\n",
    "    print(sum(1 for line in file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33b2c9c6398c50b82a2ba88e16cc7fe542e9abdb"
   },
   "source": [
    "First few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "44c4c5b29051ba666f2abf38451deba29475dc01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,MSSubClass,MSZoning,LotFrontage,LotArea,Street,Alley,LotShape,LandContour,Utilities,LotConfig,LandSlope,Neighborhood,Condition1,Condition2,BldgType,HouseStyle,OverallQual,OverallCond,YearBuilt,YearRemodAdd,RoofStyle,RoofMatl,Exterior1st,Exterior2nd,MasVnrType,MasVnrArea,ExterQual,ExterCond,Foundation,BsmtQual,BsmtCond,BsmtExposure,BsmtFinType1,BsmtFinSF1,BsmtFinType2,BsmtFinSF2,BsmtUnfSF,TotalBsmtSF,Heating,HeatingQC,CentralAir,Electrical,1stFlrSF,2ndFlrSF,LowQualFinSF,GrLivArea,BsmtFullBath,BsmtHalfBath,FullBath,HalfBath,BedroomAbvGr,KitchenAbvGr,KitchenQual,TotRmsAbvGrd,Functional,Fireplaces,FireplaceQu,GarageType,GarageYrBlt,GarageFinish,GarageCars,GarageArea,GarageQual,GarageCond,PavedDrive,WoodDeckSF,OpenPorchSF,EnclosedPorch,3SsnPorch,ScreenPorch,PoolArea,PoolQC,Fence,MiscFeature,MiscVal,MoSold,YrSold,SaleType,SaleCondition,SalePrice\n",
      "\n",
      "1,60,RL,65,8450,Pave,NA,Reg,Lvl,AllPub,Inside,Gtl,CollgCr,Norm,Norm,1Fam,2Story,7,5,2003,2003,Gable,CompShg,VinylSd,VinylSd,BrkFace,196,Gd,TA,PConc,Gd,TA,No,GLQ,706,Unf,0,150,856,GasA,Ex,Y,SBrkr,856,854,0,1710,1,0,2,1,3,1,Gd,8,Typ,0,NA,Attchd,2003,RFn,2,548,TA,TA,Y,0,61,0,0,0,0,NA,NA,NA,0,2,2008,WD,Normal,208500\n",
      "\n",
      "2,20,RL,80,9600,Pave,NA,Reg,Lvl,AllPub,FR2,Gtl,Veenker,Feedr,Norm,1Fam,1Story,6,8,1976,1976,Gable,CompShg,MetalSd,MetalSd,None,0,TA,TA,CBlock,Gd,TA,Gd,ALQ,978,Unf,0,284,1262,GasA,Ex,Y,SBrkr,1262,0,0,1262,0,1,2,0,3,1,TA,6,Typ,1,TA,Attchd,1976,RFn,2,460,TA,TA,Y,298,0,0,0,0,0,NA,NA,NA,0,5,2007,WD,Normal,181500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(iowa_file) as file:\n",
    "    for lnum, line in enumerate(file):\n",
    "        print(line)\n",
    "        if lnum > 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e56f1936cd6fcd5146acd6546bbcd03aeb96b0d"
   },
   "source": [
    "Looks good! Note that the data comes split into a train and a test portion. In practice, you'd need to run these tests on each portion but since this is a demonstration we proceed immediately with loading both files into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "a91bb59aab4b7e6ffd531499194775388a92c881"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iowa_train_data = pd.read_csv('../input/train.csv')\n",
    "iowa_test_data = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7621475c4bed1de283a7128857531a251f632b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data portion: 1460\n",
      "Length of test data portion: 1459\n"
     ]
    }
   ],
   "source": [
    "print('Length of train data portion: {}\\nLength of test data portion: {}'.format(len(iowa_train_data),\n",
    "len(iowa_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "c5431f53c62fc94a323cd451f8bb907100462774"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iowa_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0bf6c343dd46681dcec16dae4cf3f57e5fb37ecf"
   },
   "source": [
    "## Part 2: Building a Random Forest model and handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba33c1a01e204297242c2246572d52e59632c2a3"
   },
   "source": [
    "First, we define the prediction features and target. For simplicity, we'll ignore categorical (non-numerical) data for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "e146fd6ef49b654dc33611bfd4dd34f5d882605f"
   },
   "outputs": [],
   "source": [
    "iowa_target = iowa_train_data['SalePrice']\n",
    "iowa_predictors = iowa_train_data.drop(['SalePrice'], axis=1)\n",
    "\n",
    "# For the sake of keeping the example simple, we'll use only numeric predictors. \n",
    "iowa_numeric_predictors = iowa_predictors.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88d1c1c9-91ba-4bef-aee5-f4a19a68e61c",
    "_uuid": "de7bdb4f005022ea45742b5d25f47cba7a6d698d"
   },
   "source": [
    "Let's also create a function to measure the quality of an approach. This function reports the out-of-sample mean absolute error (MAE) score from a Random Forest. We begin by splitting our training data into a a train and a test portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "986dbfe4-9386-4a03-b2a4-9e99bf1b08f5",
    "_kg_hide-input": true,
    "_uuid": "6088bfdac20ece9c040e83beb28ff169d17f0666"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iowa_numeric_predictors, \n",
    "                                                    iowa_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7184fc99-e266-4bd7-af0d-0da9c96887f0",
    "_uuid": "df0103056e52ffb3c500f5fc1437bd175a41adad"
   },
   "source": [
    "We first try to handle missing values by dropping the entire column containing them and measure the resulting MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "64ae7a0a-95aa-47a7-aa29-a1589ebcbd18",
    "_uuid": "2957c4a7c4e6ed990b1406d5c88a9aa4c738b28f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from dropping columns with missing values:\n",
      "19458\n"
     ]
    }
   ],
   "source": [
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis = 1)\n",
    "reduced_X_test  = X_test.drop(cols_with_missing, axis = 1)\n",
    "print(\"Mean Absolute Error from dropping columns with missing values:\")\n",
    "print(int(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b446d3e1-0a5c-4552-8718-e07ab5f4496a",
    "_uuid": "e78f8751db60278737a388a4b85b72bed4d3f45b"
   },
   "source": [
    "We next try to replace missing values through standard imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "8cb756dc-9623-43b7-92c7-dfd87c70f450",
    "_uuid": "7f1030b598d08cd586a56d4ab33d1f99f6535784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from imputation:\n",
      "19202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "print(\"Mean Absolute Error from imputation:\")\n",
    "print(int(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74d86db6e782a18cfe286f6d55da95e88ac38d23"
   },
   "source": [
    "Our error seems smaller but the improvement is so little that a different `random_state` could easily make the error bigger. Why haven't we improved? Let's look at how many missing values there actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "56bcb82486d45a9a60975cacea3a560d75e93694"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotFrontage    189\n",
       "MasVnrArea       5\n",
       "GarageYrBlt     54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = X_train.isnull().sum()\n",
    "missing[missing != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dfe3f8be93897749d36595b96977ffdaab11b64b"
   },
   "source": [
    "There are only few missing values (remember, there are 1460 rows) and they are concentrated in only 3 out of 80 features/columns. Replacing these with their mean values (which is what `SimpleImputer` does) is thus no improvement over simply dropping the three columns. Still, let's see if keeping track of which columns have missing values that are replaced improves the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "39ba8166-9b03-41cb-9403-d728b342d5e7",
    "_uuid": "914b9e57b99d7964013f007537c300fe57e0bf91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from imputation while tracking what was imputed:\n",
      "19525\n"
     ]
    }
   ],
   "source": [
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "\n",
    "cols_with_missing = (col for col in X_train.columns if X_train[col].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n",
    "\n",
    "print(\"Mean Absolute Error from imputation while tracking what was imputed:\")\n",
    "print(int(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35888f7d87fa506a07afbb0cb57c5b489bd5f621"
   },
   "source": [
    "It does not. Again, the `random_state` variance is greater than the error difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1dc9e6072bad39ead6ba047f7909fbd0eddfe999"
   },
   "source": [
    "## Part 3: Converting categorical data to dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49669f889f61347fd132dd983c1dbb2e14fcf740"
   },
   "source": [
    "Let's see if we can improve our model by one-hot encoding our categoricals, i.e. turning them into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "873e1270d2b461d39d3816f1c6fe9fa5c2cc8a88"
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_iowa_predictors = pd.get_dummies(iowa_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "d68f071944b758f0ffda5b32de7448d3878d1f5f"
   },
   "outputs": [],
   "source": [
    "X_train_1hot, X_test_1hot, y_train, y_test = train_test_split(one_hot_encoded_iowa_predictors, \n",
    "                                                    iowa_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "\n",
    "cols_with_missing = [col for col in X_train_1hot.columns if X_train_1hot[col].isnull().any()]\n",
    "reduced_X_train_1hot = X_train_1hot.drop(cols_with_missing, axis = 1)\n",
    "reduced_X_test_1hot  = X_test_1hot.drop(cols_with_missing, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "e6406aace1eb4020d5e2552126c4afc757f44634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from including categorical data:\n",
      "18613\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Absolute Error from including categorical data:\")\n",
    "print(int(score_dataset(reduced_X_train_1hot, reduced_X_test_1hot, y_train, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "752e00ad53d4e773cf21e3f8b07a7f5bba40262d"
   },
   "source": [
    "We have reduced the error by about $1,000. This should be more than the `random_state` variance and a first small piece of progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96e7cda13c65241facf4ad673ca8001edd966754"
   },
   "source": [
    "## Part 4: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ddd653836a22b5d06e0b42038c3654a9050b144d"
   },
   "source": [
    "Let's use the powerful XGBoost regressor and run it on our data set with missing values dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "8e9246c2fd9e486de4cb587a1607647b30d106b6"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "363de4bf8ad2f2022c492738585240f486e959a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(reduced_X_train_1hot, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "de91420d4dc687e7210cbd6616afef7ae1430425",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 16380\n"
     ]
    }
   ],
   "source": [
    "xgb_preds = xgb.predict(reduced_X_test_1hot)\n",
    "print(\"Mean Absolute Error: \" + str(int(mean_absolute_error(y_test, xgb_preds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "729d3186f7f9c11c046918cf216b2fb6833c9c66"
   },
   "source": [
    "Great! We've reduced our error by another $2300. To further improve, let's find the optimal number of estimators to use in our XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "f92134279d906f09def5872fb2fdded4e2ab1542"
   },
   "outputs": [],
   "source": [
    "def get_xgb_mae(n_estimators, learning_rate=0.1):\n",
    "    xgb = XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "    xgb.fit(reduced_X_train_1hot, y_train, eval_set=[(reduced_X_test_1hot, y_test)], verbose=False)\n",
    "    xgb_preds = xgb.predict(reduced_X_test_1hot)\n",
    "    mae = mean_absolute_error(y_test, xgb_preds)\n",
    "    print(\"Mean Absolute Error from using {} estimators at a learning rate of {}: {}\"\\\n",
    "          .format(str(n_estimators), learning_rate, str(int(mae))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "f8173d2426eda140279960537521813c518c1ea9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from using 100 estimators at a learning rate of 0.1: 16380\n",
      "Mean Absolute Error from using 200 estimators at a learning rate of 0.1: 16028\n",
      "Mean Absolute Error from using 300 estimators at a learning rate of 0.1: 15967\n",
      "Mean Absolute Error from using 400 estimators at a learning rate of 0.1: 16014\n",
      "Mean Absolute Error from using 500 estimators at a learning rate of 0.1: 16080\n",
      "Mean Absolute Error from using 600 estimators at a learning rate of 0.1: 16082\n",
      "Mean Absolute Error from using 700 estimators at a learning rate of 0.1: 16061\n",
      "Mean Absolute Error from using 800 estimators at a learning rate of 0.1: 16100\n",
      "Mean Absolute Error from using 900 estimators at a learning rate of 0.1: 16159\n",
      "Mean Absolute Error from using 1000 estimators at a learning rate of 0.1: 16176\n"
     ]
    }
   ],
   "source": [
    "for i in range(100, 1100, 100):\n",
    "    get_xgb_mae(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "09c14b4813e55901f5c0d39c62041f95d102ddf6"
   },
   "source": [
    "We see that the best number of estimators is 300. By using a slower learning rate we may be able to further improve our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "7fc3cc5be1676e269f99a873794a3a656f863e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from using 300 estimators at a learning rate of 0.05: 15935\n"
     ]
    }
   ],
   "source": [
    "get_xgb_mae(300, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec152d8ee5e035fbd1b15063ad647ae88fde495a"
   },
   "source": [
    "## Part 5: Trying our model on the whole data set and submitting our work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67e48833a4feb0431c1f9a5447b2dbaed6cac326"
   },
   "source": [
    "Now that we have determined the best model and parameters, we train our model on the entire data set and submit our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "de12bfb39a99c550ca3fde23016a4aa41eebc0fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121293.23 162708.9  175002.23 ... 154983.5  122153.27 235333.69]\n"
     ]
    }
   ],
   "source": [
    "# Starting data\n",
    "iowa_target = iowa_train_data['SalePrice']\n",
    "y_train = iowa_target\n",
    "iowa_predictors = iowa_train_data.drop(['SalePrice'], axis=1)\n",
    "\n",
    "# Drop missing values from training and test data\n",
    "cols_with_missing = [col for col in iowa_predictors.columns if iowa_predictors[col].isnull().any()]\n",
    "reduced_iowa_predictors = iowa_predictors.drop(cols_with_missing, axis = 1)\n",
    "reduced_iowa_test_data = iowa_test_data.drop(cols_with_missing, axis = 1)\n",
    "\n",
    "# Convert categoricals to dummies in training and test data and aligning the columns\n",
    "one_hot_encoded_reduced_iowa_predictors = pd.get_dummies(reduced_iowa_predictors)\n",
    "one_hot_encoded_reduced_iowa_test_data = pd.get_dummies(reduced_iowa_test_data)\n",
    "X_train, X_test = one_hot_encoded_reduced_iowa_predictors.align(one_hot_encoded_reduced_iowa_test_data,\n",
    "                                                                    join='left', \n",
    "                                                                    axis=1)\n",
    "\n",
    "# Build best currently known model and use it to make predictions\n",
    "xgb = XGBRegressor(n_estimators=300, learning_rate=0.05)\n",
    "xgb.fit(X_train, y_train, verbose=False)\n",
    "predicted_prices = xgb.predict(X_test)\n",
    "\n",
    "# Print result\n",
    "print(predicted_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "845384b1ffebe278b3378c696a9a42d3554dbc3c"
   },
   "outputs": [],
   "source": [
    "# Prepare data for submission\n",
    "my_submission = pd.DataFrame({'Id': X_test.Id, 'SalePrice': predicted_prices})\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7964dbbf6e826fb2dccca64a63bd1b9c3ec7e739"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
